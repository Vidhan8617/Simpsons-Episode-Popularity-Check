# -*- coding: utf-8 -*-
"""Simpsons Episodes Popularity Check.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iz6EYQI4yv8bJLoBLIAW-cbto798Z6AH
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df_character = pd.read_csv('simpsons_characters.csv')
df_lines = pd.read_csv('simpsons_script_lines.csv', low_memory=False)
df_episode = pd.read_csv('simpsons_episodes.csv')
df_location = pd.read_csv('simpsons_locations.csv')

df_lines.info()

df_lines[df_lines.speaking_line=='false']

df_lines[df_lines.speaking_line=='false'].iloc[1]['raw_text']

df_lines = df_lines.dropna(subset=['character_id'])
df_lines.shape

df_lines.info()

df_lines['spoken_words'] = df_lines['spoken_words'].fillna('')
character_lines_count_by_episode = df_lines.groupby(['character_id', 'episode_id']).count()[['spoken_words']].reset_index()
character_lines_count_by_episode.info();

character_lines_count_by_episode.head()

"""> Then we want to merge this dataset with df_character, character_id is always more reliable than raw_character_text. For raw text there might be variation in spelling and causing problem when merge."""

df_character.info()

"""> df_character has id as int data type, and in our character_lines_count_by_episode dataframe, character_id is of data type object, so need to convert them to consistent type before merge."""

def convert_to_int(x):
    try:
        return int(x)
    except:
        return -999

character_lines_count_by_episode['character_id'] = character_lines_count_by_episode['character_id'].apply(convert_to_int)
character_lines_count_by_episode = character_lines_count_by_episode.merge(df_character, left_on='character_id', right_on='id', how='left')
character_lines_count_by_episode = character_lines_count_by_episode.drop('id', axis=1).sort_values('character_id')
character_lines_count_by_episode = character_lines_count_by_episode.rename(columns={'spoken_words':'lines_count'})

character_lines_count_by_episode = character_lines_count_by_episode.sort_values('lines_count', ascending=False)

character_lines_count_by_episode.head()

character_lines_count_by_episode[character_lines_count_by_episode.gender.isnull()]

"""> Use **pivot** function to rearrange information"""

character_lines_count_by_episode = character_lines_count_by_episode.pivot(index=['name'], columns = ['episode_id'])[['lines_count']]
character_lines_count_by_episode = character_lines_count_by_episode['lines_count'].fillna(0.0)

character_lines_count_by_episode

top_n = 300
top_characters = list(character_lines_count_by_episode.sum(axis=1).sort_values(ascending=False)[:top_n].index)
top_characters

character_lines_count_by_episode_filtered = character_lines_count_by_episode[character_lines_count_by_episode.index.isin(top_characters)]

character_lines_count_by_episode_filtered

df_episode.shape

len(df_lines.episode_id.unique())

df_episode.imdb_rating.hist(bins=30);

threshold = 7.3
df_episode['popularity_class'] = df_episode.imdb_rating>=threshold
df_episode['popularity_class'] = df_episode['popularity_class'].astype(int)

# 1 meaning rating over threshold (popular)
# 0 meaning rating below threshold (unpopular)
df_episode['popularity_class'].value_counts()

df_episode.columns

df_episode[['imdb_rating', 'imdb_votes','number_in_season']].corr()

related_cols=['number_in_season',
              'number_in_series',
              'original_air_year',
              'season',
              'us_viewers_in_millions',
              'views',
              'popularity_class']

df_episode = df_episode[related_cols]
df_episode = df_episode.reset_index().rename(columns={'index':'episode_id'})

df_episode.head()

# combine the information we have
character_lines_with_episode_info = pd.merge(character_lines_count_by_episode_filtered.T,
                                             df_episode, on='episode_id', how='outer')

character_lines_with_episode_info.shape

# Sort the columns by top_characters plus the related columns, remember the index is episode_id (total 600 episodes)
character_lines_with_episode_info[top_characters+related_cols]

"""> Note that we have NaN values in the dataset, how would you deal with them? **Imputation Methods**"""

y_all = character_lines_with_episode_info['popularity_class']
X_all = character_lines_with_episode_info.drop('popularity_class', axis=1)
X_all.shape, y_all.shape, character_lines_with_episode_info.shape

y_all.to_csv('y_all.csv', index=True)
X_all.to_csv('X_all.csv', index=True)